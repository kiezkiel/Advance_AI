{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Transformer classes for generation\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "# import torch for datatype Attributes \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set meta llma variable to hold llama2 weights naming \n",
    "name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#set auth tokenizer variable from hugging face\n",
    "auth_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating autotokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, cache_dir = './models/', use_auth_token = auth_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cbd144533545149797cd9319d7f1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(name,cache_dir = './models/', use_auth_token= auth_token, torch_dtype = torch.float32, rope_scaling={\"type\": \"dynamic\", \"factor\": 2}, load_in_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = input(\"Please enter your question \")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt= True, skip_special_tokens = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'token_type_ids' in inputs:# Check if 'token_type_ids' is present and remove it\n",
    "    del inputs['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(torch.float32)# Convert the model and inputs to 32-bit precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float32. This is not supported.\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.to(torch.float32)# Convert the model and inputs to 32-bit precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Answer: 171\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**inputs, streamer=streamer,use_cache=True, max_new_tokens=float('inf')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the output tokens back to text\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the prompt wraper but for the llama index\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "#create a system prompt\n",
    "system_promt = \"\"\"<s>[INST] <<SYS>> you are a helpful, resecful and honest assistant <</SYS>> \"\"\"\n",
    "# throw together the quesry wrapper \n",
    "query_wrapper_promt = SimpleInputPrompt(\"{query_str}[/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Konnichiwa goshojin sama[/INST]'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_wrapper_promt.format(query_str='Konnichiwa goshojin sama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imort the llama index HF wrapper\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "# create a HF LLM using the llama index wraer\n",
    "llm = HuggingFaceLLM(context_window=4096,\n",
    "                     max_new_tokens=256,\n",
    "                     system_prompt=system_promt,\n",
    "                     query_wrapper_prompt=query_wrapper_promt,\n",
    "                     model=model,\n",
    "                     tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in embeddings wraer \n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "#bring in HF embeddings-need these to represent documents chunks\n",
    "from langchain.embeddings.huggingface import  HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and dl embeddings instance \n",
    "embneddings = LangchainEmbedding(\n",
    "    HuggingFaceBgeEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in stuff to change service context\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new service context instance\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embneddings\n",
    ")\n",
    "# and set the service context actually passing service_context \n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download PDF loader\n",
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "#create PDF loader\n",
    "loader = PyMuPDFReader()\n",
    "# load documents \n",
    "documents = SimpleDirectoryReader('./datas').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index - we'll be able to query this in a sec\n",
    "index  = VectorStoreIndex.from_documents(documents, llm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rresponse = query_engine.query(\"what did the author do growing u?\")\n",
    "print(rresponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = input(\"Please enter your question \")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt= True, skip_special_tokens = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs, streamer=streamer,use_cache=True, max_new_tokens=float('inf')) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kakarotto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
